{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import utils\n",
    "import midi_statistics\n",
    "import os\n",
    "import time\n",
    "import mmd\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "# import torch\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "torch.backends.cuda.enable_math_sdp(True)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "REG_CONSTANT = 0.1\n",
    "DISABLE_FEED_PREVIOUS = True\n",
    "GAUSSIAN_NOISE = False\n",
    "UNIDIRECTIONAL_D = True\n",
    "BIDIRECTIONAL_G = False\n",
    "RANDOM_INPUT_SCALE = 44\n",
    "ATTENTION_LENGTH = 0\n",
    "FEED_COND_D = True\n",
    "RANDOM_INPUT_DIM = 100  # Adjusted to make the input dimension divisible by NUM_HEADS_G\n",
    "DROPOUT_PROB  = 0.1\n",
    "D_LR_FACTOR = 0.3\n",
    "LEARNING_RATE = 1e-4\n",
    "PRETRAINING_D = False\n",
    "LR_DECAY = 0.98\n",
    "DATA_MATRIX = \"data/processed_dataset_matrices/full_data_matrix.npy\"\n",
    "TRAIN_DATA_MATRIX = \"data/processed_dataset_matrices/train_data_matrix.npy\"\n",
    "VALIDATE_DATA_MATRIX = \"data/processed_dataset_matrices/valid_data_matrix.npy\"\n",
    "TEST_DATA_MATRIX = \"data/processed_dataset_matrices/test_data_matrix.npy\"\n",
    "LOSS_WRONG_D = False\n",
    "INPUT_VECTOR = \"input_vector.npy\"\n",
    "MULTI = True\n",
    "CONDITION = True\n",
    "SONGLENGTH = 20\n",
    "PRETRAINING_EPOCHS = 1\n",
    "NUM_MIDI_FEATURES = 3\n",
    "NUM_SYLLABLE_FEATURES = 20\n",
    "NUM_SONGS = 5000000\n",
    "BATCH_SIZE = 512\n",
    "REG_SCALE = 1.0\n",
    "TRAIN_RATE = 0.8\n",
    "VALIDATION_RATE = 0.1\n",
    "HIDDEN_SIZE_G = 512\n",
    "HIDDEN_SIZE_D = 256\n",
    "NUM_LAYERS_G = 4\n",
    "NUM_LAYERS_D = 4\n",
    "ADAM = True\n",
    "DISABLE_L2_REG = True\n",
    "MAX_GRAD_NORM = 5.0\n",
    "FEATURE_MATCHING = True\n",
    "MAX_EPOCH = 450\n",
    "EPOCHS_BEFORE_DECAY = 30\n",
    "SONGLENGTH_CEILING = 20\n",
    "\n",
    "# Set number of heads\n",
    "NUM_HEADS_G = 8\n",
    "NUM_HEADS_D = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Create a matrix of [max_len, d_model] representing the positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # Compute the positional encodings once in log space.\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # Apply sine to even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # Apply cosine to odd indices\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)  # Shape: [max_len, 1, d_model]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [sequence_length, batch_size, d_model]\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_song_features, num_meta_features, songlength, conditioning='multi'):\n",
    "        super(Generator, self).__init__()\n",
    "        self.songlength = songlength\n",
    "        self.num_song_features = num_song_features\n",
    "        self.num_meta_features = num_meta_features\n",
    "        self.conditioning = conditioning\n",
    "\n",
    "        # Transformer parameters\n",
    "        self.num_heads = NUM_HEADS_G\n",
    "        self.dropout = DROPOUT_PROB\n",
    "        self.generator_dropout = nn.Dropout(p=DROPOUT_PROB)\n",
    "        # Generator layers\n",
    "        input_size_generator = RANDOM_INPUT_DIM + (num_meta_features if conditioning == 'multi' else 0)\n",
    "        self.generator_input_linear = nn.Linear(input_size_generator, HIDDEN_SIZE_G)\n",
    "        self.generator_pos_encoder = PositionalEncoding(HIDDEN_SIZE_G, max_len=songlength)\n",
    "        encoder_layer_generator = nn.TransformerEncoderLayer(\n",
    "            d_model=HIDDEN_SIZE_G,\n",
    "            nhead=self.num_heads,\n",
    "            dim_feedforward=HIDDEN_SIZE_G * 4,\n",
    "            dropout=0.1,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.generator_transformer = nn.TransformerEncoder(\n",
    "            encoder_layer_generator,\n",
    "            num_layers=NUM_LAYERS_G\n",
    "        )\n",
    "        self.generator_output_linear = nn.Linear(HIDDEN_SIZE_G, num_song_features)\n",
    "\n",
    "    def forward(self, batch_size, conditioning_data=None, training=True):\n",
    "        device = next(self.parameters()).device\n",
    "        random_input = torch.randn(\n",
    "            (batch_size, self.songlength, RANDOM_INPUT_DIM),\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        if self.conditioning == 'multi' and conditioning_data is not None:\n",
    "            generator_input = torch.cat([random_input, conditioning_data], dim=-1)\n",
    "        else:\n",
    "            generator_input = random_input\n",
    "\n",
    "\n",
    "        if training and DROPOUT_PROB  < 1.0:\n",
    "            generator_input = self.generator_dropout(generator_input)\n",
    "        # Project input to model dimension\n",
    "        generator_input = self.generator_input_linear(generator_input)\n",
    "        generator_input = generator_input.transpose(0, 1)  # [seq_len, batch_size, model_dim]\n",
    "\n",
    "        # Add positional encoding\n",
    "        generator_input = self.generator_pos_encoder(generator_input)\n",
    "\n",
    "        if training and DROPOUT_PROB  < 1.0:\n",
    "            generator_input = F.dropout(generator_input, p=self.dropout, training=training)\n",
    "\n",
    "        # Transformer expects [sequence_length, batch_size, model_dim]\n",
    "        gen_output = self.generator_transformer(generator_input)\n",
    "\n",
    "        # Transform back to [batch_size, sequence_length, model_dim]\n",
    "        gen_output = gen_output.transpose(0, 1)\n",
    "        generated_features = self.generator_output_linear(gen_output)\n",
    "        return generated_features\n",
    "\n",
    "# Define the Discriminator class\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_song_features, num_meta_features, songlength, conditioning='multi'):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.songlength = songlength\n",
    "        self.num_song_features = num_song_features\n",
    "        self.num_meta_features = num_meta_features\n",
    "        self.conditioning = conditioning\n",
    "\n",
    "        # Transformer parameters\n",
    "        self.num_heads = NUM_HEADS_D\n",
    "        self.dropout = DROPOUT_PROB \n",
    "        self.discriminator_dropout = nn.Dropout(p=DROPOUT_PROB )\n",
    "        # Discriminator layers\n",
    "        input_size_discriminator = num_song_features + (num_meta_features if conditioning == 'multi' and FEED_COND_D else 0)\n",
    "        self.discriminator_input_linear = nn.Linear(input_size_discriminator, HIDDEN_SIZE_D)\n",
    "        self.discriminator_pos_encoder = PositionalEncoding(HIDDEN_SIZE_D, max_len=songlength)\n",
    "        encoder_layer_discriminator = nn.TransformerEncoderLayer(\n",
    "            d_model=HIDDEN_SIZE_D,\n",
    "            nhead=self.num_heads,\n",
    "            dim_feedforward=HIDDEN_SIZE_D * 4,\n",
    "            dropout=0.1,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.discriminator_transformer = nn.TransformerEncoder(\n",
    "            encoder_layer_discriminator,\n",
    "            num_layers=NUM_LAYERS_D\n",
    "        )\n",
    "        self.discriminator_output_linear = nn.Linear(HIDDEN_SIZE_D, 1)\n",
    "        # Remove self.discriminator_sigmoid\n",
    "\n",
    "    def forward(self, song_data, conditioning_data=None, training=True):\n",
    "        # device = next(self.parameters()).device\n",
    "\n",
    "\n",
    "        if self.conditioning == 'multi' and conditioning_data is not None and FEED_COND_D:\n",
    "            discriminator_input = torch.cat([song_data, conditioning_data], dim=-1)\n",
    "        else:\n",
    "            discriminator_input = song_data\n",
    "\n",
    "\n",
    "        # Project input to model dimension\n",
    "        discriminator_input = self.discriminator_input_linear(discriminator_input)\n",
    "        discriminator_input = discriminator_input.transpose(0, 1)  # [seq_len, batch_size, model_dim]\n",
    "\n",
    "        # Add positional encoding\n",
    "        discriminator_input = self.discriminator_pos_encoder(discriminator_input)\n",
    "\n",
    "        if training and DROPOUT_PROB  < 1.0:\n",
    "            discriminator_input = F.dropout(discriminator_input, p=self.dropout, training=training)\n",
    "\n",
    "        # Transformer expects [sequence_length, batch_size, model_dim]\n",
    "        disc_output = self.discriminator_transformer(discriminator_input)\n",
    "\n",
    "        # Transform back to [batch_size, sequence_length, model_dim]\n",
    "        disc_output = disc_output.transpose(0, 1)\n",
    "\n",
    "        # Pass through output layer\n",
    "        decision = self.discriminator_output_linear(disc_output)\n",
    "        decision = decision.mean(dim=1).squeeze()\n",
    "        return decision\n",
    "\n",
    "\n",
    "def compute_gradient_penalty(discriminator, real_samples, fake_samples, conditioning_data):\n",
    "    batch_size = real_samples.size(0)\n",
    "    device = real_samples.device\n",
    "\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = torch.rand(batch_size, 1, 1, device=device)\n",
    "    alpha = alpha.expand_as(real_samples)\n",
    "\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolated = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "\n",
    "    # Forward pass through discriminator\n",
    "    interpolated_output = discriminator(interpolated, conditioning_data)\n",
    "\n",
    "    # Compute gradients w.r.t. the interpolated outputs\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=interpolated_output,\n",
    "        inputs=interpolated,\n",
    "        grad_outputs=torch.ones_like(interpolated_output),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "\n",
    "    # Compute gradient norm\n",
    "    gradients = gradients.reshape(batch_size, -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "\n",
    "    # Compute gradient penalty\n",
    "    gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "def discriminator_loss(real_output, fake_output, gradient_penalty_weight, lambda_gp=10):\n",
    "    return fake_output.mean() - real_output.mean() + lambda_gp * gradient_penalty_weight\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return -fake_output.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(generator, discriminator, song_data, conditioning_data, batch_size, generator_optimizer=None, discriminator_optimizer=None):\n",
    "    device = next(generator.parameters()).device\n",
    "    song_data = song_data.to(device)\n",
    "    if conditioning_data is not None:\n",
    "        conditioning_data = conditioning_data.to(device)\n",
    "    gen_loss = None\n",
    "    disc_loss = None\n",
    "\n",
    "    # =======================\n",
    "    # Train Discriminator\n",
    "    # =======================\n",
    "    if discriminator_optimizer is not None:\n",
    "        discriminator.train()\n",
    "        discriminator_optimizer.zero_grad()\n",
    "\n",
    "        # Generate fake songs\n",
    "        with torch.no_grad():\n",
    "            fake_songs = generator(batch_size, conditioning_data, training=True)\n",
    "\n",
    "        # Compute discriminator outputs\n",
    "        real_output = discriminator(song_data, conditioning_data)\n",
    "        fake_output = discriminator(fake_songs, conditioning_data)\n",
    "\n",
    "        # Compute gradient penalty\n",
    "        gradient_penalty = compute_gradient_penalty(discriminator, song_data, fake_songs, conditioning_data)\n",
    "\n",
    "        # Compute discriminator loss\n",
    "        disc_loss = discriminator_loss(real_output, fake_output, gradient_penalty)\n",
    "\n",
    "        # Backward and optimize\n",
    "        disc_loss.backward()\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "    # =======================\n",
    "    # Train Generator\n",
    "    # =======================\n",
    "    if generator_optimizer is not None:\n",
    "        generator.train()\n",
    "        generator_optimizer.zero_grad()\n",
    "\n",
    "        # Generate fake songs\n",
    "        fake_songs = generator(batch_size, conditioning_data, training=True)\n",
    "\n",
    "        # Compute discriminator output on fake songs\n",
    "        fake_output = discriminator(fake_songs, conditioning_data)\n",
    "\n",
    "        # Compute generator loss\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "\n",
    "        # Backward and optimize\n",
    "        gen_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "    return gen_loss.item() if gen_loss is not None else None, disc_loss.item() if disc_loss is not None else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu_score(references, candidates):\n",
    "    # references: لیستی از لیست‌های مرجع\n",
    "    # candidates: لیست نمونه‌های تولید شده\n",
    "    bleu_scores = []\n",
    "    for ref, cand in zip(references, candidates):\n",
    "        score = sentence_bleu([ref], cand)\n",
    "        bleu_scores.append(score)\n",
    "    return np.mean(bleu_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data\n",
    "train = np.load(TRAIN_DATA_MATRIX)\n",
    "validate = np.load(VALIDATE_DATA_MATRIX)\n",
    "test = np.load(TEST_DATA_MATRIX)\n",
    "\n",
    "if not CONDITION:\n",
    "    train = train[:, :SONGLENGTH * NUM_MIDI_FEATURES]\n",
    "    validate = validate[:, :SONGLENGTH * NUM_MIDI_FEATURES]\n",
    "    test = test[:, :SONGLENGTH * NUM_MIDI_FEATURES]\n",
    "\n",
    "print(\"Training set: \", train.shape[0], \" songs\")\n",
    "print(\"Validation set: \", validate.shape[0], \" songs\")\n",
    "print(\"Test set: \", test.shape[0], \" songs\")\n",
    "\n",
    "# Create dataset using torch.utils.data.Dataset and DataLoader\n",
    "if CONDITION:\n",
    "    class MusicDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, data):\n",
    "            self.song_data = data[:, :SONGLENGTH * NUM_MIDI_FEATURES].reshape(-1, SONGLENGTH, NUM_MIDI_FEATURES)\n",
    "            self.conditioning_data = data[:, SONGLENGTH * NUM_MIDI_FEATURES:].reshape(-1, SONGLENGTH, NUM_SYLLABLE_FEATURES)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.song_data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            song = self.song_data[idx].astype(np.float32)\n",
    "            condition = self.conditioning_data[idx].astype(np.float32)\n",
    "            # Create wrong conditioning data\n",
    "            wrong_idx = np.random.randint(len(self.song_data))\n",
    "            wrong_condition = self.conditioning_data[wrong_idx].astype(np.float32)\n",
    "            return song, condition, wrong_condition\n",
    "    dataset = MusicDataset(train)\n",
    "else:\n",
    "    class MusicDataset(torch.utils.data.Dataset):\n",
    "        def __init__(self, data):\n",
    "            self.song_data = data[:, :SONGLENGTH * NUM_MIDI_FEATURES].reshape(-1, SONGLENGTH, NUM_MIDI_FEATURES)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.song_data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            song = self.song_data[idx].astype(np.float32)\n",
    "            return song\n",
    "    dataset = MusicDataset(train)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "# Initialize models\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"RUN ON \", device)\n",
    "generator = Generator(\n",
    "    num_song_features=NUM_MIDI_FEATURES,\n",
    "    num_meta_features=NUM_SYLLABLE_FEATURES,\n",
    "    songlength=SONGLENGTH,\n",
    "    conditioning='multi',\n",
    ").to(device)\n",
    "\n",
    "discriminator = Discriminator(\n",
    "    num_song_features=NUM_MIDI_FEATURES,\n",
    "    num_meta_features=NUM_SYLLABLE_FEATURES,\n",
    "    songlength=SONGLENGTH,\n",
    "    conditioning='multi',\n",
    ").to(device)\n",
    "\n",
    "# Set up optimizers\n",
    "if ADAM:\n",
    "    generator_optimizer = optim.Adam(\n",
    "        generator.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        betas=(0.0, 0.9 ),\n",
    "        # weight_decay=1e-5,\n",
    "    )\n",
    "    discriminator_optimizer = optim.Adam(\n",
    "        discriminator.parameters(),\n",
    "        lr=LEARNING_RATE * D_LR_FACTOR,\n",
    "        betas=(0.0, 0.9),\n",
    "        # weight_decay=1e-5,\n",
    "    )\n",
    "else:\n",
    "    generator_optimizer = optim.SGD(generator.parameters(), lr=LEARNING_RATE)\n",
    "    discriminator_optimizer = optim.SGD(discriminator.parameters(), lr=LEARNING_RATE * D_LR_FACTOR)\n",
    "\n",
    "# Learning rate schedulers\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < EPOCHS_BEFORE_DECAY:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return LR_DECAY ** (epoch - EPOCHS_BEFORE_DECAY + 1)\n",
    "\n",
    "# generator_scheduler = optim.lr_scheduler.LambdaLR(generator_optimizer, lr_lambda=lr_lambda)\n",
    "# discriminator_scheduler = optim.lr_scheduler.LambdaLR(discriminator_optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# Load checkpoint if exists\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'model_checkpoint.pth')\n",
    "start_epoch = 0\n",
    "best_mmd_overall = np.inf\n",
    "best_epoch = 0\n",
    "if os.path.exists(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
    "    generator_optimizer.load_state_dict(checkpoint['generator_optimizer_state_dict'])\n",
    "    discriminator_optimizer.load_state_dict(checkpoint['discriminator_optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    best_mmd_overall = checkpoint['best_mmd_overall']\n",
    "    best_epoch = checkpoint['best_epoch']\n",
    "    print(f\"Loaded model from checkpoint at epoch {start_epoch}\")\n",
    "n_critic = 5\n",
    "last_G_loss = 0\n",
    "last_D_loss = 0\n",
    "\n",
    "gen_losses_per_epoch = []\n",
    "disc_losses_per_epoch = []\n",
    "mmd_overall_list = []\n",
    "bleu_scores_per_epoch = []\n",
    "\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(start_epoch, MAX_EPOCH):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Adjust learning rates\n",
    "    # if epoch >= EPOCHS_BEFORE_DECAY:\n",
    "    #     generator_scheduler.step()\n",
    "    #     discriminator_scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{MAX_EPOCH}, Learning Rate: {generator_optimizer.param_groups[0]['lr']:.5f}\")\n",
    "\n",
    "    gen_losses = []\n",
    "    disc_losses = []\n",
    "\n",
    "    for batch_data in tqdm(dataloader):\n",
    "        if CONDITION:\n",
    "            song_data, conditioning_data, wrong_conditioning_data = batch_data\n",
    "        else:\n",
    "            song_data = batch_data\n",
    "            conditioning_data = None\n",
    "            wrong_conditioning_data = None\n",
    "\n",
    "        batch_size = song_data.size(0)\n",
    "        for _ in range(n_critic):\n",
    "            gen_loss, disc_loss = train_step(\n",
    "                generator,\n",
    "                discriminator,\n",
    "                song_data,\n",
    "                conditioning_data,\n",
    "                batch_size,\n",
    "                generator_optimizer=generator_optimizer,\n",
    "                discriminator_optimizer=discriminator_optimizer,\n",
    "            )\n",
    "        gen_loss, _ = train_step(\n",
    "            generator,\n",
    "            discriminator,\n",
    "            song_data,\n",
    "            conditioning_data,\n",
    "            batch_size,\n",
    "            generator_optimizer=generator_optimizer,\n",
    "            discriminator_optimizer=None,  # No discriminator update\n",
    "        )\n",
    "        gen_losses.append(gen_loss)\n",
    "        if disc_loss is not None:\n",
    "            disc_losses.append(disc_loss)\n",
    "\n",
    "    gLoss = np.mean(gen_losses)\n",
    "    gen_losses_per_epoch.append(gLoss)\n",
    "    \n",
    "    print(f\"Average Generator Loss: {gLoss:.8f}, Different From last: {(last_G_loss - gLoss):.8f}\")\n",
    "    if disc_losses:\n",
    "        dLoss = np.mean(disc_losses)\n",
    "        disc_losses_per_epoch.append(dLoss)\n",
    "        print(f\"Average Discriminator Loss: {dLoss:.8f}, Different From last: {(last_D_loss - dLoss):.8f}\")\n",
    "        last_D_loss = dLoss\n",
    "\n",
    "    last_G_loss = gLoss\n",
    "    # Save model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'generator_state_dict': generator.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "                'generator_optimizer_state_dict': generator_optimizer.state_dict(),\n",
    "                'discriminator_optimizer_state_dict': discriminator_optimizer.state_dict(),\n",
    "                'best_mmd_overall': best_mmd_overall,\n",
    "                'best_epoch': best_epoch,\n",
    "            },\n",
    "            checkpoint_path,\n",
    "        )\n",
    "        print(f\"Model saved at epoch {epoch + 1}\")\n",
    "\n",
    "    # Evaluation and MMD calculation\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    validation_songs = []\n",
    "    \n",
    "    generated_songs = []\n",
    "    real_songs = []\n",
    "    \n",
    "            # Initialize lists to store metrics\n",
    "    midi_numbers_span_list = []\n",
    "    repetitions_3_list = []\n",
    "    repetitions_2_list = []\n",
    "    unique_midi_numbers_list = []\n",
    "    notes_without_rest_list = []\n",
    "    average_rest_value_list = []\n",
    "    song_length_list = []\n",
    "    mmdLast = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(validate)):\n",
    "            # Prepare conditioning data\n",
    "            if CONDITION:\n",
    "                conditioning_data = validate[i, SONGLENGTH * NUM_MIDI_FEATURES:].reshape(\n",
    "                    1, SONGLENGTH, NUM_SYLLABLE_FEATURES\n",
    "                )\n",
    "                conditioning_data = torch.tensor(conditioning_data, dtype=torch.float32).to(device)\n",
    "            else:\n",
    "                conditioning_data = None\n",
    "\n",
    "            # Generate song\n",
    "            generated_features = generator(1, conditioning_data, training=False)\n",
    "            sample = generated_features.cpu().numpy().squeeze(0)\n",
    "            discretized_sample = utils.discretize(sample)\n",
    "            discretized_sample = np.array(discretized_sample)\n",
    "            validation_songs.append(discretized_sample)\n",
    "            \n",
    "            generated_songs.extend(generated_features.cpu().numpy())\n",
    "            real_songs.extend(song_data.cpu().numpy())       \n",
    "            \n",
    "            # Compute metrics for the current song\n",
    "            midi_numbers = discretized_sample[:, 0]  # Assuming the first column is MIDI note numbers\n",
    "            rest_values = discretized_sample[:, 2]   # Assuming the third column is rest durations\n",
    "\n",
    "            # MIDI Numbers Span\n",
    "            midi_span = midi_numbers.max() - midi_numbers.min()\n",
    "            midi_numbers_span_list.append(midi_span)\n",
    "\n",
    "            # Repetitions of 3-MIDI numbers\n",
    "            repetitions_3 = midi_statistics.count_repetitions(midi_numbers, n=3)\n",
    "            repetitions_3_list.append(repetitions_3)\n",
    "\n",
    "            # Repetitions of 2-MIDI numbers\n",
    "            repetitions_2 = midi_statistics.count_repetitions(midi_numbers, n=2)\n",
    "            repetitions_2_list.append(repetitions_2)\n",
    "\n",
    "            # Number of Unique MIDI numbers\n",
    "            unique_midi_numbers = len(np.unique(midi_numbers))\n",
    "            unique_midi_numbers_list.append(unique_midi_numbers)\n",
    "\n",
    "            # Number of Notes Without Rest\n",
    "            notes_without_rest = np.sum(rest_values == 0)\n",
    "            notes_without_rest_list.append(notes_without_rest)\n",
    "\n",
    "            # Average Rest Value Within Song\n",
    "            average_rest = np.mean(rest_values)\n",
    "            average_rest_value_list.append(average_rest)\n",
    "\n",
    "            # Song Length\n",
    "            song_length = len(midi_numbers)\n",
    "            song_length_list.append(song_length)\n",
    "\n",
    "\n",
    "        generated_sequences = [list(map(int, song.flatten())) for song in generated_songs]\n",
    "        reference_sequences = [list(map(int, song.flatten())) for song in real_songs]   \n",
    "        bleu_score = calculate_bleu_score(reference_sequences, generated_sequences)\n",
    "        bleu_scores_per_epoch.append(bleu_score)\n",
    "        print(f\"BLEU Score on validation set: {bleu_score:.4f}\")\n",
    "        \n",
    "        # Compute MMD\n",
    "        val_gen_pitches = np.array([song[:, 0] for song in validation_songs])\n",
    "        val_dat_pitches = validate[:, : NUM_MIDI_FEATURES * SONGLENGTH : NUM_MIDI_FEATURES]\n",
    "        MMD_pitch = mmd.Compute_MMD(val_gen_pitches, val_dat_pitches)\n",
    "        print(\"MMD pitch:\", MMD_pitch)\n",
    "\n",
    "        val_gen_duration = np.array([song[:, 1] for song in validation_songs])\n",
    "        val_dat_duration = validate[:, 1 : NUM_MIDI_FEATURES * SONGLENGTH : NUM_MIDI_FEATURES]\n",
    "        MMD_duration = mmd.Compute_MMD(val_gen_duration, val_dat_duration)\n",
    "        print(\"MMD duration:\", MMD_duration)\n",
    "\n",
    "        val_gen_rests = np.array([song[:, 2] for song in validation_songs])\n",
    "        val_dat_rests = validate[:, 2 : NUM_MIDI_FEATURES * SONGLENGTH : NUM_MIDI_FEATURES]\n",
    "        MMD_rest = mmd.Compute_MMD(val_gen_rests, val_dat_rests)\n",
    "        print(\"MMD rest:\", MMD_rest)\n",
    "\n",
    "        MMD_overall = MMD_pitch + MMD_duration + MMD_rest\n",
    "        print(\"MMD overall:\", MMD_overall)\n",
    "        print(f\"Different From Last: {(MMD_overall - mmdLast):.5f}\")\n",
    "        print(f\"Different From Best: {(best_mmd_overall - MMD_overall):.5f}\")\n",
    "        mmd_overall_list.append(MMD_overall)\n",
    "        mmdLast = MMD_overall\n",
    "# After processing all songs, compute the average metrics\n",
    "        avg_midi_span = np.mean(midi_numbers_span_list)\n",
    "        avg_repetitions_3 = np.mean(repetitions_3_list)\n",
    "        avg_repetitions_2 = np.mean(repetitions_2_list)\n",
    "        avg_unique_midi_numbers = np.mean(unique_midi_numbers_list)\n",
    "        avg_notes_without_rest = np.mean(notes_without_rest_list)\n",
    "        avg_average_rest_value = np.mean(average_rest_value_list)\n",
    "        avg_song_length = np.mean(song_length_list)\n",
    "        print(\"=====Midi stats-----\")\n",
    "        # Print the metrics\n",
    "        print(f\"Average MIDI Numbers Span: {avg_midi_span:.1f}\")\n",
    "        print(f\"Average 3-MIDI Numbers Repetitions: {avg_repetitions_3:.1f}\")\n",
    "        print(f\"Average 2-MIDI Numbers Repetitions: {avg_repetitions_2:.1f}\")\n",
    "        print(f\"Average Number of Unique MIDI: {avg_unique_midi_numbers:.1f}\")\n",
    "        print(f\"Average Number of Notes Without Rest: {avg_notes_without_rest:.1f}\")\n",
    "        print(f\"Average Rest Value Within Song: {avg_average_rest_value:.1f}\")\n",
    "        print(f\"Average Song Length: {avg_song_length:.1f}\")\n",
    "        end_time = time.time()\n",
    "        print(f\"Time for epoch {epoch + 1}: {end_time - start_time:.2f} seconds\")\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(gen_losses_per_epoch, label='Generator Loss')\n",
    "        plt.plot(disc_losses_per_epoch, label='Discriminator Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Generator and Discriminator Loss Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # نمودار BLEU Score\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(bleu_scores_per_epoch, label='BLEU Score')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('BLEU Score')\n",
    "        plt.title('BLEU Score Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # در صورت وجود، نمودار MMD Overall\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(mmd_overall_list, label='MMD Overall')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MMD')\n",
    "        plt.title('MMD Overall Over Epochs')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        print(\"==========================@@@@===========================\")\n",
    "        # Save best model based on MMD overall\n",
    "        if MMD_overall < best_mmd_overall:\n",
    "            print(f\"Best model at epoch {epoch + 1} with MMD overall: {MMD_overall:.5f}\")\n",
    "            best_mmd_overall = MMD_overall\n",
    "            best_epoch = epoch + 1\n",
    "            torch.save({\n",
    "                'generator_state_dict': generator.state_dict(),\n",
    "                'discriminator_state_dict': discriminator.state_dict(),\n",
    "            }, './saved_gan_models/best_model.pth')\n",
    "\n",
    "\n",
    "print(f\"Best model at epoch {best_epoch} with MMD overall: {best_mmd_overall:.5f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "-1.-1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
